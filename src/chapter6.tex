% !TEX root = ../swputhesis.tex
\chapter{附录}
1. 特征提取网络
\begin{lstlisting}	%正文插入代码
    import torch
    import torch.nn as nn
    class PixelwiseFeatureExtractor(nn.Module):
        def __init__(self):
            super(PixelwiseFeatureExtractor, self).__init__()
            # 定义卷积层
            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)
            self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
            self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)
            self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)
            self.conv5 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)
            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
            self.relu = nn.ReLU(inplace=True)
            self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)   
        def forward(self, x):
            x1 = self.conv1(x)
            x1 = self.relu(x1)
            x1 = self.pool(x1)
            x2 = self.conv2(x1)
            x2 = self.relu(x2)
            x2 = self.pool(x2)
            x3 = self.conv3(x2)
            x3 = self.relu(x3)
            x3 = self.pool(x3)
            
            x4 = self.conv4(x3)
            x4 = self.relu(x4)
            x4 = self.pool(x4)
            x5 = self.conv5(x4)
            x5 = self.relu(x5)
            x5_up = self.upsample(x5)
            x4_up = self.conv4(x3)
            x4_up = self.relu(x4_up)
            x4_up = torch.cat([x4_up, x5_up], dim=1)
            x4_up = self.upsample(x4_up)
            x3_up = self.conv3(x2)
            x3_up = self.relu(x3_up)
            x3_up = torch.cat([x3_up, x4_up], dim=1)
            x3_up = self.upsample(x3_up)
            x2_up = self.conv2(x1)
            x2_up = self.relu(x2_up)
            x2_up = torch.cat([x2_up, x3_up], dim=1)
            x2_up = self.upsample(x2_up)
            x1_up = self.conv1(x)
            x1_up = self.relu(x1_up)
            x1_up = torch.cat([x1_up, x2_up], dim=1)
            return x1_up
\end{lstlisting}

2.语义分支网络
\begin{lstlisting}	%正文插入代码

    import tensorflow as tf
    def fcn_model(input_shape, num_classes):
        # 定义输入层
        inputs = tf.keras.layers.Input(shape=input_shape)
        # 编码器部分
        conv1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
        pool1 = tf.keras.layers.MaxPooling2D((2, 2))(conv1)
        conv2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)
        pool2 = tf.keras.layers.MaxPooling2D((2, 2))(conv2)
        conv3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)
        # 解码器部分
        up1 = tf.keras.layers.UpSampling2D((2, 2))(conv3)
        conv4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(up1)
        up2 = tf.keras.layers.UpSampling2D((2, 2))(conv4)
        conv5 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(up2)
        outputs = tf.keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv5)
\end{lstlisting}


3.全景分支网络
\begin{lstlisting}
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    class PanopticSegNet(nn.Module):
        def __init__(self, num_classes):
            super(PanopticSegNet, self).__init__()
            self.num_classes = num_classes
            
            # Encoder
            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
            self.bn1 = nn.BatchNorm2d(64)
            self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
            self.bn2 = nn.BatchNorm2d(128)
            self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
            self.bn3 = nn.BatchNorm2d(256)
            self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
            self.bn4 = nn.BatchNorm2d(512)
            self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)
            self.bn5 = nn.BatchNorm2d(1024)
            # Decoder
            self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)
            self.bn6 = nn.BatchNorm2d(512)
            self.conv6 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)
            self.bn7 = nn.BatchNorm2d(512)
            self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
            self.bn8 = nn.BatchNorm2d(256)
            self.conv7 = nn.Conv2d(512, 256, kernel_size=3, padding=1)
            self.bn9 = nn.BatchNorm2d(256)
            self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
            self.bn10 = nn.BatchNorm2d(128)
            self.conv8 = nn.Conv2d(256, 128, kernel_size=3, padding=1)
            self.bn11 = nn.BatchNorm2d(128)
            self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
            self.bn12 = nn.BatchNorm2d(64)
            self.conv9 = nn.Conv2d(128, 64, kernel_size=3, padding=1)
            self.bn13 = nn.BatchNorm2d(64)
            # Output
            self.conv10 = nn.Conv2d(64, self.num_classes, kernel_size=1)
        def forward(self, x):
            # Encoder
            x = F.relu(self.bn1(self.conv1(x)))
            x = F.max_pool2d(x, 2)
            x = F.relu(self.bn2(self.conv2(x)))
            x = F.max_pool2d(x, 2)
            x = F.relu(self.bn3(self.conv3(x)))
            x = F.max_pool2d(x, 2)
            x = F.relu(self.bn4(self.conv4(x)))
            x = F.max_pool2d(x, 2)
            x = F.relu(self.bn5(self.conv5(x)))
            # Decoder
            x = F.relu(self.bn6(self.upconv1(x)))
            x = torch.cat([x, self.bn4(self.conv4(x))], dim=1)
            x = F.relu(self.bn7(self.conv6(x)))
            x = F.relu(self.bn8(self.upconv2(x)))
            x = torch.cat([x, self.bn3(self.conv3(x))], dim=1)
            x = F.relu(self.bn9(self.conv7(x)))
            x = F.relu(self.bn10(self.upconv3(x)))
            x = torch.cat([x, self.bn2(self.conv2(x))], dim=1)
            x = F.relu(self.bn11(self.conv8(x)))
            x = F.relu(self.bn12(self.upconv4(x)))
            x = torch.cat([x, self.bn1(self.conv1(x))], dim=1)
            x = F.relu(self.bn13(self.conv9(x)))
            # Output
            x = self.conv10(x)
            x = F.interpolate(x, scale_factor=4, mode='bilinear')
            return x
\end{lstlisting}

4.导入必要的库和模块
\begin{lstlisting}
    import tensorflow as tf
    from tensorflow.keras.applications.resnet50 import ResNet50
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Conv2DTranspose, Concatenate, Activation 
\end{lstlisting}

5.定义网络模型
\begin{lstlisting}
    def create_model(input_shape, num_classes):
        # 加载ResNet50-FPN模型
        resnet50_fpn = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)
        # 获取输出特征图
        C3, C4, C5 = resnet50_fpn.get_layer('conv3_block4_out').output, resnet50_fpn.get_layer('conv4_block6_out').output, resnet50_fpn.output
        # 上采样
        P5 = Conv2DTranspose(filters=256, kernel_size=(3, 3), strides=2, padding='same')(C5)
        P5 = Concatenate()([P5, C4])
        P5 = Conv2DTranspose(filters=256, kernel_size=(3, 3), strides=2, padding='same')(P5)
        P5 = Concatenate()([P5, C3])
        # 分类器
        output = Conv2DTranspose(filters=num_classes, kernel_size=(3, 3), strides=2, padding='same')(P5)
        output = Activation('softmax')(output)
        # 定义模型
        model = Model(inputs=resnet50_fpn.input, outputs=output)
        return model
\end{lstlisting}

6.训练编译模型
\begin{lstlisting}
    model = create_model(input_shape=(256, 256, 3), num_classes=2)
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))
\end{lstlisting}

